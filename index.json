
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I am an applied scientist at Amazon Q Developer. I was on the academic job market! Here are my statements: [Research Statement], [Teaching Statement], [Diversity Statement].\nWelcome! I\u0026#39;m Yuhao. I got my Ph.D. in computer science at the University of Wisconsin‚ÄìMadison, conducting research at the madPL group, under the guidance of Prof. Loris D\u0026#39;Antoni and Prof. Aws Albarghouthi. I received my B.S. in Computer Science at Peking University (PKU) in 2019, mentored by Prof. Yingfei Xiong. I am good at algorithms and data structures. I was a contestant of ACM-ICPC and I had won four gold medals at Regional and China-Final contests. When I was in high school, I won a gold medal at the National Olympiad of Informatics.\nMy primary research focuses on the fields of software engineering, programming languages, and deep learning. The goal of my research is to provide provable guarantees for deep learning, ensuring its safety, security, and reliability via formal methods. I am also interested in integrating program synthesis with deep learning techniques, such as large language models, to generate programs in a interpretable and verifiable manner. ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://foreverzyh.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am an applied scientist at Amazon Q Developer. I was on the academic job market! Here are my statements: [Research Statement], [Teaching Statement], [Diversity Statement].\nWelcome! I'm Yuhao. I got my Ph.","tags":null,"title":"Yuhao Zhang","type":"authors"},{"authors":null,"categories":null,"content":"","date":1718388000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718388000,"objectID":"fed3c7874427bf649c2a2807aad1c818","permalink":"https://foreverzyh.github.io/news/2024_06_14/","publishdate":"2024-06-14T12:00:00-06:00","relpermalink":"/news/2024_06_14/","section":"news","summary":"","tags":null,"title":"I graduated with a PhD in Computer Science","type":"news"},{"authors":null,"categories":null,"content":"","date":1714435200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714435200,"objectID":"54ca2e6e3af4f4e6d155847e968bada3","permalink":"https://foreverzyh.github.io/publication/pl_ml_arct/","publishdate":"2024-04-30T00:00:00Z","relpermalink":"/publication/pl_ml_arct/","section":"publication","summary":"","tags":null,"title":"A One-Layer Decoder-Only Transformer is a Two-Layer RNN, With an Application to Certified Robustness","type":"publication"},{"authors":null,"categories":null,"content":"","date":1713830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713830400,"objectID":"4f4674160c147a972d35f8d447e6eb58","permalink":"https://foreverzyh.github.io/publication/se_ml_codefort/","publishdate":"2024-04-23T00:00:00Z","relpermalink":"/publication/se_ml_codefort/","section":"publication","summary":"","tags":null,"title":"CodeFort, Robust Training for Code Generation Models","type":"publication"},{"authors":null,"categories":null,"content":"","date":1711152000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711152000,"objectID":"4b05cc8d4fecb26b6039e91c000f35f1","permalink":"https://foreverzyh.github.io/publication/pl_ml_pecan/","publishdate":"2024-03-23T00:00:00Z","relpermalink":"/publication/pl_ml_pecan/","section":"publication","summary":"","tags":null,"title":"PECAN, A Deterministic Certified Defense Against Backdoor Attacks","type":"publication"},{"authors":null,"categories":null,"content":"","date":1709683200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709683200,"objectID":"3a86fc3888bfa8e8645bf2ae3f826efe","permalink":"https://foreverzyh.github.io/publication/pl_ml_vericfx/","publishdate":"2024-03-06T00:00:00Z","relpermalink":"/publication/pl_ml_vericfx/","section":"publication","summary":"","tags":null,"title":"Verified Training for Counterfactual Explanation Robustness under Data Shift","type":"publication"},{"authors":null,"categories":null,"content":"","date":1701388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701388800,"objectID":"ee2f20b40d8c1e5575e8b18780214e41","permalink":"https://foreverzyh.github.io/services/neurips/","publishdate":"2023-12-01T00:00:00Z","relpermalink":"/services/neurips/","section":"services","summary":"","tags":null,"title":"NeurIPS 2023, 2022, Reviewer","type":"services"},{"authors":null,"categories":null,"content":"","date":1700404200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700404200,"objectID":"13c5f40e335a7a954610b4e97a1b76bc","permalink":"https://foreverzyh.github.io/news/2023_11_20/","publishdate":"2023-11-19T08:30:00-06:00","relpermalink":"/news/2023_11_20/","section":"news","summary":"","tags":null,"title":"I am a top reviewer at NeurIPS 2023","type":"news"},{"authors":null,"categories":null,"content":"","date":1700352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700352000,"objectID":"c01bbc1222e7e617b5e469c6b26d6331","permalink":"https://foreverzyh.github.io/award/top_reviewer23/","publishdate":"2023-11-19T00:00:00Z","relpermalink":"/award/top_reviewer23/","section":"award","summary":"Recognized as a [top reviewer](https://neurips.cc/Conferences/2023/ProgramCommittee) for NeurIPS 2023 (2023, NeurIPS)","tags":null,"title":"top reviewer","type":"award"},{"authors":null,"categories":null,"content":"","date":1696602600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696602600,"objectID":"7e8713022a735c9d58a225826cea74a4","permalink":"https://foreverzyh.github.io/news/2023_10_06/","publishdate":"2023-10-06T08:30:00-06:00","relpermalink":"/news/2023_10_06/","section":"news","summary":"","tags":null,"title":"I give a talk at [MWPLS 2023](https://mwpls2023.engin.umich.edu).","type":"news"},{"authors":null,"categories":null,"content":"üßó I fell in love with indoor bouldering during my Ph.D. and have climbed once to twice a week since May 2022. I won the third place in the beginner category of the 2023 climbing competition in UW-Madison! I‚Äôm tackling V3-V5 routes and will continue improving and practicing in the coming years.\n","date":1696118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696118400,"objectID":"1d13a2ab0cd81c4e6749258bf7780ed1","permalink":"https://foreverzyh.github.io/others/climb/","publishdate":"2023-10-01T00:00:00Z","relpermalink":"/others/climb/","section":"others","summary":"üßó I fell in love with indoor bouldering during my Ph.D. and have climbed once to twice a week since May 2022. I won the third place in the beginner category of the 2023 climbing competition in UW-Madison!","tags":null,"title":"some things","type":"others"},{"authors":null,"categories":null,"content":"When I‚Äôm not hanging from cliffs, I usually hit the gym about three to four times a week since September 2021. After cutting nearly 20 pounds of pandemic weight gain, I embarked on a body-fat-cutting journey in April 2023. I achieved the single-digit-body-fat goal in September of the same year and shifted to clean bulking mode afterward.\n","date":1696118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696118400,"objectID":"b12835c9d48a11bd9da55e7bb3da5960","permalink":"https://foreverzyh.github.io/others/gym/","publishdate":"2023-10-01T00:00:00Z","relpermalink":"/others/gym/","section":"others","summary":"When I‚Äôm not hanging from cliffs, I usually hit the gym about three to four times a week since September 2021. After cutting nearly 20 pounds of pandemic weight gain, I embarked on a body-fat-cutting journey in April 2023.","tags":null,"title":"some things","type":"others"},{"authors":null,"categories":null,"content":"But it‚Äôs not all about physical pursuits for me. I‚Äôve been immersing myself in (non-CS) books since September 2021, dedicating more than one hour every day to reading. I love reading novels by Fyodor Dostoevsky and writers Lationamericano like as Mario Vargas Llosa and Gabriel Garc√≠a M√°rquez. On the other side, I delved into Lacanian Psychoanalysis and am currently studying the German Idealism Philosophy of Hegel. My journey won‚Äôt stop there; I‚Äôm eager to explore Phenomenology, courtesy of Husserl and Merleau-Ponty, and (post-) structuralism critique of ideology by Althusser.\n","date":1696118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696118400,"objectID":"f62aa0b787c6991213aafa77523edc5d","permalink":"https://foreverzyh.github.io/others/read/","publishdate":"2023-10-01T00:00:00Z","relpermalink":"/others/read/","section":"others","summary":"But it‚Äôs not all about physical pursuits for me. I‚Äôve been immersing myself in (non-CS) books since September 2021, dedicating more than one hour every day to reading. I love reading novels by Fyodor Dostoevsky and writers Lationamericano like as Mario Vargas Llosa and Gabriel Garc√≠a M√°rquez.","tags":null,"title":"some things","type":"others"},{"authors":null,"categories":null,"content":"","date":1696032000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696032000,"objectID":"418e4f906221c266bf3d57f6ce1bf330","permalink":"https://foreverzyh.github.io/award/travel_award_mwpls2023/","publishdate":"2023-09-30T00:00:00Z","relpermalink":"/award/travel_award_mwpls2023/","section":"award","summary":"Midwest Programming Languages Summit Travel Awards (2023, MWPLS)","tags":null,"title":"mwpls travel award","type":"award"},{"authors":null,"categories":null,"content":"This project designs certifiable defenses against data poisoning and backdoor attacks during training. High-quality, abundant data is crucial for training deep learning models to address complex problems. However, the integrity of this data is threatened by data poisoning attacks, where an attacker can subtly modify the training set to manipulate the model‚Äôs predictions. Such attacks have been successfully utilized to surreptitiously insert backdoors into deep learning models.\nThis concern for data-poisoning attacks on deep learning models has led to my research on certified defenses ensuring the integrity of deep learning model training.\nPapers: BagFlip (NeurIPS2022), PECAN (Under Submission)\nKey ideas:\nProbabilistic and deterministic certified defenses against training-time attacks. A holistic view of handling test-time and training-time threats. Certified defenses against test-time and training-time attacks in a holistic view I have proposed two certified defenses, BagFlip and PECAN, against data poisoning attacks that can modify both the training set and test inputs. These defenses construct a verifiable training algorithm over the original algorithm by creating an ensemble of models, each trained on subsets of the training data. These defenses adopt a holistic view of inference and training processes by regarding these processes as a closed box, abstracting away the intricate details of the training algorithm, which can pose challenges for verification techniques in establishing meaningful bounds. Leveraging this holistic view, BagFlip employs randomized smoothing to construct probabilistic proofs. In contrast, PECAN generates deterministic proofs by seamlessly integrating training-time and test-time proofs derived from corresponding techniques.\n","date":1693699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693699200,"objectID":"d3f983500316ca577193414a55aa74c7","permalink":"https://foreverzyh.github.io/project/project_1/","publishdate":"2023-09-03T00:00:00Z","relpermalink":"/project/project_1/","section":"project","summary":"This project designs certifiable defenses against data poisoning and backdoor attacks during training. High-quality, abundant data is crucial for training deep learning models to address complex problems. However, the integrity of this data is threatened by data poisoning attacks, where an attacker can subtly modify the training set to manipulate the model‚Äôs predictions.","tags":null,"title":"Integrity of Deep Learning Model Training","type":"project"},{"authors":null,"categories":null,"content":"This project proves and improves the robustness of deep learning models against inference-time adversarial examples. A unique characteristic of deep learning models is their vulnerability to malicious attacks, even when the underlying code implementations are correct. Among the various types of attacks, inference-time (or test-time) attacks have been extensively studied as they directly affect the performance and reliability of the model. These attacks craft a human-imperceptible perturbation to the test input to deceive the model into making incorrect predictions.\nTest-time defenses and attacks on deep learning models have been a never-ending cat-and-mouse game. My research aims to end this game by providing deep learning model inference with well-defined and provable guarantees. I focus on the robustness verification of language models, an area previously unexplored due to the challenge of the discreteness of the inputs.\nPapers: A3T (ICML2020), ARC (EMNLP2021)\nKey ideas:\nLanguages for describing test-time robustness for deep learning models. Training approaches for improving model robustness. An abstract interpretation technique for verifying model robustness. Programmable perturbation space Existing work on robustness for deep learning model inference employs ad-hoc perturbations tailored to specific attacks, such as synonym substitutions. However, these perturbations do not apply to a wide range of scenarios. To address this limitation, I introduced the concept of a programmable perturbation space and designed a language for defining attacks/perturbations to input sequences for language models. The versatile language allows users to express their specific robustness requirements as user-defined string transformations and combinations. For example, it can express a perturbation that removes stop words and duplicates exclamation and question marks in a movie review. Furthermore, this language enables robustness verification and training approaches to compile and understand users‚Äô needs seamlessly.\nVerifying robustness of recursive models Given a robustness specification as a programmable perturbation space, my approach, ARC, generates proofs of robustness for recursive models, such as LSTMs or Tree-LSTMs. The key idea underlying ARC involves symbolically recursive memoization and abstraction of sets of possible hidden states, a task that becomes infeasible for enumeration due to its exponential growth with the input length. As ARC over-approximates the sets of all possible outcomes, it captures the worst-case scenario, thus establishing proofs for model robustness.\nRobust training approaches When given a programmable perturbation space, the challenge of training robust models against the space lies in accurately approximating the worst-case loss. Traditional approximation methods provide loose approximations, such as the under-approximation by adversarial training or the over-approximation by provable training. To overcome this challenge, I proposed A3T, an innovative approach that approximates the worst-case by decomposing the programmable perturbation space into two subsets: one that can be explored using adversarial training and another that can be abstracted using provable training. This novel idea of decomposition has been adopted by the state-of-the-art robust training method, SABR.\n","date":1693612800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693612800,"objectID":"13c691720a8605e74d914f8fec455dec","permalink":"https://foreverzyh.github.io/project/project_2/","publishdate":"2023-09-02T00:00:00Z","relpermalink":"/project/project_2/","section":"project","summary":"This project proves and improves the robustness of deep learning models against inference-time adversarial examples. A unique characteristic of deep learning models is their vulnerability to malicious attacks, even when the underlying code implementations are correct.","tags":null,"title":"Robustness of Deep Learning Model Inference","type":"project"},{"authors":null,"categories":null,"content":"This project ensures the correctness of deep learning training and inference codes built on top of TensorFlow/PyTorch. Deep learning models have made remarkable advancements in solving complex and diverse problems. The training and inference of these models are coded built on platforms like TensorFlow and PyTorch. This coding requires a programming paradigm that differs significantly from traditional ones. For instance, programming logic relies more heavily on intensive matrix operations and activation functions than traditional programming structures such as branches and loops. As a result, new defects have emerged in deep learning code, and we need to rethink the problem of verifying code correctness and providing provably correct fixes to these defects.\nIn my research on providing provable guarantees for the correctness of deep learning code, I pose the following questions: What are these new defects? Which defect type is of most concern to the deep learning community? The first question led me to conduct a pioneering empirical study of novel defects in deep learning code. This study found that numerical defects, which often manifest as NaN or INF during neural network computations, are prevalent in deep learning code and can significantly impair the model accuracy, potentially leading to system crashes. This finding addressed the second question and guided my work on ensuring deep learning code is free of numerical defects.\nDEBAR: A static analyzer over the computational graph. RANUM: A framework for reliability assurance. Papers: An empirical study (ISSTA2018), DEBAR (FSE2020), RANUM (ICSE2023)\nKey ideas:\nA pioneering empirical study of defects in deep learning code. An abstract interpretation technique for verifying the absence of numerical defects. A framework for reliability assurance against numerical defects by providing failure-exhibiting tests and provably correct fix suggestions. A pioneering empirical study Motivated by the need to investigate emerging defects in deep learning code, I conducted a pioneering empirical study on the root causes, symptoms, and fixing strategies of these new defects. This empirical study not only serves as a bedrock of my work but also inspires numerous initiatives in the research community. For example, our study identified new defects such as unaligned-shape defects, where the shape of a tensor, a data structure predominantly used in deep learning, does not align with its expected shape. Subsequently, a corresponding approach has been proposed to detect this new defect. Additionally, Islam et al. conducted a further empirical study and ‚Äúadapted the classification scheme of root causes and bug effects‚Äù from our study.\nVerification of the absence of numerical defects My work, DEBAR, detects numerical defects in deep learning code. It can either construct a proof confirming the absence of numerical defects within the code or identify suspicious computational graph nodes that may have numerical defects. To construct such proof, I design refined yet scalable abstract domains, tensor partitioning and interval with affine equality relation, to over-approximate the output range of each node in the computational graph. If a node receives an invalid range or its output overflows, DEBAR will report this node as suspicious. And if no such node exists in the code, DEBAR then gets a correctness proof owing to the soundness of abstract interpretation. DEBAR is highly scalable as it only takes an average of 12.1 seconds for each model, while maintaining a low false positive rate of 7.7%. DEBAR has received recognition with the ACM SIGSOFT Distinguished Paper Award at FSE 2020 and it detected 11 real-world bugs of 48 models implemented in an official repository maintained by the TensorFlow team (See, e.g., defect 1, defect 2).\nProvably correct fixes to numerical defects Once the defects are identified, developers need to fix them. We found that developers either provided no fixes or designed heuristic fixes that did not eliminate these defects, such as reducing the learning rate. Such heuristic fixes often delay the triggering of numerical defects during training and further obscure the defects. Our work, RANUM, automatically synthesizes provably correct fixes to these defects for developers. The fixes take the form of clipping the input ranges of some computational graph nodes. Striking the right balance with these input ranges is critical; overly tight ranges can hinder the model accuracy, whereas overly wide ranges may still result in numerical defects. We propose an abstraction optimization algorithm to find the tightest input ranges that provably eliminate the defects.\n","date":1693526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693526400,"objectID":"482b03b1cd1950497ab767037c9acf41","permalink":"https://foreverzyh.github.io/project/project_3/","publishdate":"2023-09-01T00:00:00Z","relpermalink":"/project/project_3/","section":"project","summary":"This project ensures the correctness of deep learning training and inference codes built on top of TensorFlow/PyTorch. Deep learning models have made remarkable advancements in solving complex and diverse problems. The training and inference of these models are coded built on platforms like TensorFlow and PyTorch.","tags":null,"title":"Correctness of Codes using Deep Learning Platforms","type":"project"},{"authors":null,"categories":null,"content":"","date":1690588800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690588800,"objectID":"eda9aac29b15f51de7231f26f7fca819","permalink":"https://foreverzyh.github.io/news/2023_07_28/","publishdate":"2023-07-28T14:00:00-10:00","relpermalink":"/news/2023_07_28/","section":"news","summary":"","tags":null,"title":"I give a joint talk with my advisor Aws at [WFVML 23](https://www.ml-verification.com/invited-speakers).","type":"news"},{"authors":null,"categories":null,"content":"","date":1690070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690070400,"objectID":"746d69406cd22d658252f8ab16f28a1f","permalink":"https://foreverzyh.github.io/services/icml/","publishdate":"2023-07-23T00:00:00Z","relpermalink":"/services/icml/","section":"services","summary":"","tags":null,"title":"ICML 2023, 2022, 2021, Reviewer","type":"services"},{"authors":null,"categories":null,"content":"","date":1689638400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689638400,"objectID":"d64de4eb891131f5bba3567c36dc923c","permalink":"https://foreverzyh.github.io/services/wfvml/","publishdate":"2023-07-18T00:00:00Z","relpermalink":"/services/wfvml/","section":"services","summary":"","tags":null,"title":"WFVML 2023, Reviewer","type":"services"},{"authors":null,"categories":null,"content":"","date":1689552000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689552000,"objectID":"bdfd3c35c2186c1b0efc2a3cfbba42df","permalink":"https://foreverzyh.github.io/services/fomlas/","publishdate":"2023-07-17T00:00:00Z","relpermalink":"/services/fomlas/","section":"services","summary":"","tags":null,"title":"FoMLAS 2023, 2022, 2021, Program Committee","type":"services"},{"authors":null,"categories":null,"content":"","date":1684762200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684762200,"objectID":"8b562da23187aa1f0905e67c705fbd2e","permalink":"https://foreverzyh.github.io/news/2023_05_22/","publishdate":"2023-05-22T08:30:00-05:00","relpermalink":"/news/2023_05_22/","section":"news","summary":"","tags":null,"title":"I start my summer internship at AWS CodeWhisperer.","type":"news"},{"authors":null,"categories":null,"content":"","date":1683655200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683655200,"objectID":"d29a9f96676f52aaa426f113955e07e0","permalink":"https://foreverzyh.github.io/news/2023_05_09/","publishdate":"2023-05-09T12:00:00-06:00","relpermalink":"/news/2023_05_09/","section":"news","summary":"","tags":null,"title":"I pass my Ph.D. Preliminary Exam.","type":"news"},{"authors":null,"categories":null,"content":"","date":1683244800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683244800,"objectID":"1cc0ca78346801fdb25e69e06b9cc1c3","permalink":"https://foreverzyh.github.io/services/bands/","publishdate":"2023-05-05T00:00:00Z","relpermalink":"/services/bands/","section":"services","summary":"","tags":null,"title":"BANDS 2023, Program Committee","type":"services"},{"authors":null,"categories":null,"content":"","date":1673827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673827200,"objectID":"5383006d0dfdb94aae592316b249a7ca","permalink":"https://foreverzyh.github.io/services/vmcai/","publishdate":"2023-01-16T00:00:00Z","relpermalink":"/services/vmcai/","section":"services","summary":"","tags":null,"title":"VMCAI 2023, Artifact Evaluation Committee","type":"services"},{"authors":null,"categories":null,"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669852800,"objectID":"b5db65cdefcea534792b3d8aa5f971e7","permalink":"https://foreverzyh.github.io/publication/se_ml_icse2023/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/publication/se_ml_icse2023/","section":"publication","summary":"","tags":null,"title":"Reliability Assurance for Deep Neural Network Architectures Against Numerical Defects","type":"publication"},{"authors":null,"categories":null,"content":"","date":1669766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669766400,"objectID":"6cb6907c78a2183ee55406a1b43c9b96","permalink":"https://foreverzyh.github.io/award/top_reviewer22/","publishdate":"2022-11-30T00:00:00Z","relpermalink":"/award/top_reviewer22/","section":"award","summary":"Recognized as a [top reviewer](https://neurips.cc/Conferences/2022/ProgramCommittee) for NeurIPS 2022 (2022, NeurIPS)","tags":null,"title":"top reviewer","type":"award"},{"authors":null,"categories":null,"content":"","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667260800,"objectID":"a7c01fc9873fa61ea13354140374785f","permalink":"https://foreverzyh.github.io/publication/pl_ml_neurips2022/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/publication/pl_ml_neurips2022/","section":"publication","summary":"","tags":null,"title":"BagFlip: A Certified Defense Against Data Poisoning","type":"publication"},{"authors":null,"categories":null,"content":"","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667260800,"objectID":"b1259206455894ca7542f73902991adc","permalink":"https://foreverzyh.github.io/publication/pl_oopsla2023/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/publication/pl_oopsla2023/","section":"publication","summary":"","tags":null,"title":"Overwatch: Learning Patterns in Code Edit Sequences","type":"publication"},{"authors":null,"categories":null,"content":"","date":1647388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647388800,"objectID":"16e84f19ea18e274545910d66f970edd","permalink":"https://foreverzyh.github.io/award/twosigma/","publishdate":"2022-03-16T00:00:00Z","relpermalink":"/award/twosigma/","section":"award","summary":"Selected as a (top-ten) finalist for Two Sigma PhD Fellowship (2022, Two Sigma)","tags":null,"title":"two sigma","type":"award"},{"authors":null,"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"c0e5fa79256c64f5badfcf7afb013e51","permalink":"https://foreverzyh.github.io/publication/pl_ml_emnlp2021/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/publication/pl_ml_emnlp2021/","section":"publication","summary":"","tags":null,"title":"Certified Robustness to Programmable Transformations in LSTMs","type":"publication"},{"authors":null,"categories":null,"content":"","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625097600,"objectID":"b7bc8f7e57dd19236a60509bd46fb099","permalink":"https://foreverzyh.github.io/services/cav/","publishdate":"2021-07-01T00:00:00Z","relpermalink":"/services/cav/","section":"services","summary":"","tags":null,"title":"CAV 2021, Artifact Evaluation Committee","type":"services"},{"authors":null,"categories":null,"content":"","date":1606694400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606694400,"objectID":"ecc6ab427ac669dcc4f179fe6fa7a224","permalink":"https://foreverzyh.github.io/award/distinguished_paper/","publishdate":"2020-11-30T00:00:00Z","relpermalink":"/award/distinguished_paper/","section":"award","summary":"ACM SIGSOFT Distinguished Paper Award (2020, FSE)","tags":null,"title":"distinguished paper","type":"award"},{"authors":null,"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"e005a0a5460caef1cd8599a9848a5af0","permalink":"https://foreverzyh.github.io/publication/se_ml_fse2020/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/publication/se_ml_fse2020/","section":"publication","summary":"","tags":null,"title":"Detecting Numerical Bugs in Neural Network Architectures","type":"publication"},{"authors":null,"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"ce6582b96470f19c459f7aebf2c4a64e","permalink":"https://foreverzyh.github.io/publication/pl_ml_icml2020/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/pl_ml_icml2020/","section":"publication","summary":"","tags":null,"title":"Robustness to Programmable String Transformations via Augmented Abstract Training","type":"publication"},{"authors":null,"categories":null,"content":"","date":1561852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561852800,"objectID":"c9a0e64e092d27427bad8ce9e2b2b0c1","permalink":"https://foreverzyh.github.io/award/undergrad_thesis/","publishdate":"2019-06-30T00:00:00Z","relpermalink":"/award/undergrad_thesis/","section":"award","summary":"Top 10 undergraduate thesis (2019, School of EECS, Peking University)","tags":null,"title":"undergrad thesis","type":"award"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"d2226538b9291931bd44452842cd2bbb","permalink":"https://foreverzyh.github.io/award/scholarship_2019/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/award/scholarship_2019/","section":"award","summary":"SenseTime Scholarship (2019, SenseTime)","tags":null,"title":"scholarship senseTime","type":"award"},{"authors":null,"categories":null,"content":"","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"28a560a9f1fcbcb54f3a54b5d7a88d83","permalink":"https://foreverzyh.github.io/publication/se_ml_issta2018/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/publication/se_ml_issta2018/","section":"publication","summary":"","tags":null,"title":"An Empirical Study on TensorFlow Program Bugs","type":"publication"},{"authors":null,"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"37feacaaaeaef6abc68b69af2551e839","permalink":"https://foreverzyh.github.io/award/scholarship_2018/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/award/scholarship_2018/","section":"award","summary":"Suzhou Industrial Park Scholarship (2018, Peking University)","tags":null,"title":"scholarship Suzhou Industrial Park","type":"award"},{"authors":null,"categories":null,"content":"","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"045eb1450acbaa8eb7398b3d29600ec4","permalink":"https://foreverzyh.github.io/award/acm_icpc_hcm_2017/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/award/acm_icpc_hcm_2017/","section":"award","summary":"ACM-ICPC 4-th place (2017, Asia Pacific Regional, Ho-Chi-Minh City Site)","tags":null,"title":"acm icpc Ho-Chi-Minh City 2017","type":"award"},{"authors":null,"categories":null,"content":"","date":1506816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506816000,"objectID":"22c68fc1c890ab3fa8695dacf1a3b877","permalink":"https://foreverzyh.github.io/award/acm_icpc_xian_2017/","publishdate":"2017-10-01T00:00:00Z","relpermalink":"/award/acm_icpc_xian_2017/","section":"award","summary":"ACM-ICPC gold medal (2017, Asia East Continent Regional, Xi'an Site)","tags":null,"title":"acm icpc xian 2017","type":"award"},{"authors":null,"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"038fda8b2ff463dc6bddf3e2c4baee9e","permalink":"https://foreverzyh.github.io/award/scholarship_2017/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/award/scholarship_2017/","section":"award","summary":"Schlumberger Scholarship (2017, School of EECS, Peking University)","tags":null,"title":"scholarship Schlumberger","type":"award"},{"authors":null,"categories":null,"content":"","date":1480636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480636800,"objectID":"8a97e9bec266e1b272681614547d3977","permalink":"https://foreverzyh.github.io/award/acm_icpc_chinafinal_2016/","publishdate":"2016-12-02T00:00:00Z","relpermalink":"/award/acm_icpc_chinafinal_2016/","section":"award","summary":"ACM-ICPC gold medal (2016, Asia East Continent Final, Shanghai, China)","tags":null,"title":"acm icpc chinafinal 2016","type":"award"},{"authors":null,"categories":null,"content":"","date":1480550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480550400,"objectID":"f0c9430ad83ac925f7f60e6e7496c6b6","permalink":"https://foreverzyh.github.io/award/acm_icpc_yangon_2016/","publishdate":"2016-12-01T00:00:00Z","relpermalink":"/award/acm_icpc_yangon_2016/","section":"award","summary":"ACM-ICPC 7-th place (2016, Asia Pacific Regional, Yangon Site)","tags":null,"title":"acm icpc Yangon 2016","type":"award"},{"authors":null,"categories":null,"content":"","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477958400,"objectID":"956a1e602bdf7ff0d7a49176838cfe9a","permalink":"https://foreverzyh.github.io/award/acm_icpc_dalian_2016/","publishdate":"2016-11-01T00:00:00Z","relpermalink":"/award/acm_icpc_dalian_2016/","section":"award","summary":"ACM-ICPC gold medal (2016, Asia East Continent Regional, Dalian Site)","tags":null,"title":"acm icpc dalian 2016","type":"award"},{"authors":null,"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"025f2a43b90943f5b2391773d78a583b","permalink":"https://foreverzyh.github.io/award/scholarship_2016/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/award/scholarship_2016/","section":"award","summary":"iPinYou Scholarship (2016, School of EECS, Peking University)","tags":null,"title":"scholarship iPinYou","type":"award"},{"authors":null,"categories":null,"content":"","date":1446336000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1446336000,"objectID":"08a91a4c49a81565bcea1cf74164c62a","permalink":"https://foreverzyh.github.io/award/acm_icpc_hefei_2015/","publishdate":"2015-11-01T00:00:00Z","relpermalink":"/award/acm_icpc_hefei_2015/","section":"award","summary":"ACM-ICPC gold medal (2015, Asia East Continent Regional, Hefei Site)","tags":null,"title":"acm icpc hefei 2015","type":"award"},{"authors":null,"categories":null,"content":"","date":951868800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":951868800,"objectID":"2545f2497ac4ca16d203ae709b976f83","permalink":"https://foreverzyh.github.io/services/cpt/","publishdate":"2000-03-01T00:00:00Z","relpermalink":"/services/cpt/","section":"services","summary":"","tags":null,"title":"Wrote a [tutorial](/uploads/CPT.html) on requesting CPT for international students in the CS department at UW-Madison.","type":"services"}]