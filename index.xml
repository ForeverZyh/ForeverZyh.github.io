<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yuhao Zhang (å¼ ç…œçš“)</title>
    <link>https://foreverzyh.github.io/</link>
      <atom:link href="https://foreverzyh.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Yuhao Zhang (å¼ ç…œçš“)</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://foreverzyh.github.io/media/icon_hua00f88fad60b7868af5bfc9120ceaf0a_569795_512x512_fill_lanczos_center_3.png</url>
      <title>Yuhao Zhang (å¼ ç…œçš“)</title>
      <link>https://foreverzyh.github.io/</link>
    </image>
    
    <item>
      <title>NeurIPS 2023, 2022, Reviewer</title>
      <link>https://foreverzyh.github.io/services/neurips/</link>
      <pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/services/neurips/</guid>
      <description></description>
    </item>
    
    <item>
      <title>I will give a talk at [MWPLS 2023](https://mwpls2023.engin.umich.edu).</title>
      <link>https://foreverzyh.github.io/news/2023_10_06/</link>
      <pubDate>Fri, 06 Oct 2023 08:30:00 -0600</pubDate>
      <guid>https://foreverzyh.github.io/news/2023_10_06/</guid>
      <description></description>
    </item>
    
    <item>
      <title>some things</title>
      <link>https://foreverzyh.github.io/others/climb/</link>
      <pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/others/climb/</guid>
      <description>&lt;p&gt;ðŸ§— I fell in love with indoor bouldering during my Ph.D. and have climbed once to twice a week since May 2022.
I won the third place in the beginner category of the 2023 climbing competition in UW-Madison!
I&amp;rsquo;m tackling V3-V5 routes and will continue improving and practicing in the coming years.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>some things</title>
      <link>https://foreverzyh.github.io/others/gym/</link>
      <pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/others/gym/</guid>
      <description>&lt;p&gt;When I&amp;rsquo;m not hanging from cliffs, I usually hit the gym about three to four times a week since September 2021. After cutting nearly 20 pounds of pandemic weight gain, I embarked on a body-fat-cutting journey in April 2023. I achieved the single-digit-body-fat goal in September of the same year and shifted to clean bulking mode afterward.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>some things</title>
      <link>https://foreverzyh.github.io/others/read/</link>
      <pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/others/read/</guid>
      <description>&lt;p&gt;But it&amp;rsquo;s not all about physical pursuits for me. I&amp;rsquo;ve been immersing myself in (non-CS) books since September 2021,
dedicating more than one hour every day to reading. I love reading novels by Fyodor Dostoevsky and writers Lationamericano
like as Mario Vargas Llosa and Gabriel GarcÃ­a MÃ¡rquez. On the other side, I delved into Lacanian Psychoanalysis and am currently studying the German Idealism Philosophy of Hegel. My journey won&amp;rsquo;t stop there; I&amp;rsquo;m eager to explore Phenomenology, courtesy of Husserl and Merleau-Ponty, and (post-) structuralism critique of ideology by Althusser.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>mwpls travel award</title>
      <link>https://foreverzyh.github.io/award/travel_award_mwpls2023/</link>
      <pubDate>Sat, 30 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/award/travel_award_mwpls2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Integrity of Deep Learning Model Training</title>
      <link>https://foreverzyh.github.io/project/project_1/</link>
      <pubDate>Sun, 03 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/project/project_1/</guid>
      <description>&lt;h4&gt;This thrust studies the problem of designing certifiable defenses against data poisoning and backdoor attacks during training.&lt;/h4&gt;
&lt;p&gt;Empowering deep learning models to tackle a wide array of complex problems hinges on their training with ample high-quality data.
However, the threat of data poisoning attacks raises a significant concern, as the attacker can modify the training set to manipulate the prediction of the victim model.
Such attacks have been exploited to insert backdoors into deep learning models surreptitiously.&lt;/p&gt;
&lt;p&gt;The concern for data-poisoning attacks on deep learning models has led to the third thrust of my research on certified defenses for ensuring the integrity of deep learning model training.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://foreverzyh.github.io/uploads/training_overview.png&#34; alt=&#34;overview&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Papers:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2205.13634&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BagFlip (NeurIPS2022)&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2301.11824&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PECAN (Under Submission)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key ideas&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Probabilistic and deterministic certified defenses against training-time attacks.&lt;/li&gt;
&lt;li&gt;A holistic view of handling test-time and training-time threats.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Certified defenses against training-time attacks&lt;/strong&gt;
I have proposed two model-agnostic certified defenses, BagFlip and PECAN, against training-time attacks that modify the training set and test inputs but do not control the training algorithm.
For a given training set, whether it is potentially poisoned or not, and a training algorithm, these defenses construct a verifiable training algorithm on top of the given algorithm by creating an ensemble of models, each trained on subsets of the training data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Holistic view of test and training&lt;/strong&gt;
These approaches take a holistic view of test and training, abstracting away the intricate details of the training algorithm, which can pose challenges for verification techniques in establishing meaningful bounds.
Leveraging this holistic view, BagFlip employs randomized smoothing to construct probabilistic proofs. In contrast, PECAN generates deterministic proofs by seamlessly integrating training-time and test-time proofs derived from corresponding techniques.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Robustness of Deep Learning Model Inference</title>
      <link>https://foreverzyh.github.io/project/project_2/</link>
      <pubDate>Sat, 02 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/project/project_2/</guid>
      <description>&lt;h4&gt;This thrust studies the problem of proving and improving the robustness of deep learning models against inference-time adversarial examples.&lt;/h4&gt;
&lt;p&gt;One distinctive nature of deep learning models is their vulnerability to malicious attacks, even when the underlying program, compiler, and system implementations are correct.
Among the various attack types, inference-time (or test-time) attacks have received extensive study.
These attacks craft a human-imperceptible perturbation to the test input to deceive the model into making incorrect predictions.&lt;/p&gt;
&lt;p&gt;Test-time defenses and attacks on deep learning models have been a never-ending cat-mouse game.
My research in this thrust focuses on ending this game by providing deep learning model inference with provable and well-defined guarantees.&lt;/p&gt;
&lt;div style=&#34;display: flex; justify-content: center; align-items: center;&#34;&gt;
  &lt;img src=&#34;https://foreverzyh.github.io/uploads/inference_a3t.png&#34; alt=&#34;A3T&#34; style=&#34;width: 50%; padding: 10px;&#34;&gt;
  &lt;img src=&#34;https://foreverzyh.github.io/uploads/inference_arc.png&#34; alt=&#34;ARC&#34; style=&#34;width: 50%; padding: 10px;&#34;&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Papers:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2002.09579&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A3T (ICML2020)&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2102.07818&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ARC (EMNLP2021)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key ideas:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Languages for describing test-time robustness for deep learning models.&lt;/li&gt;
&lt;li&gt;Training approaches that improve the test-time model robustness.&lt;/li&gt;
&lt;li&gt;Use abstract interpretation to verify whether models are robust to attacks.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Programmable perturbation space&lt;/strong&gt;
Existing work on robustness for deep learning model inference focuses on specific attacks.
Such robustness often caters to particular needs but is not universally applicable.
To cover a broader spectrum of needs, I introduced the idea of programmable perturbation space, a language for defining attacks/perturbations to input sequences (for language models).
The versatile language empowers users to express their specific robustness requirements as user-defined string transformations and combinations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Robust training approaches&lt;/strong&gt;
I have proposed A3T, an approach that trains robust models to a given programmable perturbation space.
Computing the worst-case loss of a perturbation space is infeasible because the space can be enormous and, therefore, impractical to enumerate.
The key idea of A3T is to approximate the worst-case by decomposing the programmable perturbation space into two subsets, one that can be explored using gradient-based search and one that can be abstracted using provable training such as ARC.
This idea of decomposition has inspired the state-of-the-art robust training method &lt;a href=&#34;https://arxiv.org/abs/2210.04871&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SABR&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Verifying robustness of recursive models&lt;/strong&gt;
In pursuit of providing formal guarantees for model robustness, I have introduced the ARC approach.
ARC is designed to generate proofs of the robustness of recursive models, such as LSTMs or Tree-LSTMs, against attacks specified in the programmable perturbation space.
The key idea underlying ARC involves recursive memoization and abstraction of sets of possible hidden states, a task that becomes infeasible for enumeration due to its exponential growth with the input sequence length.
As ARC over-approximates the sets of all possible outcomes, it captures the worst-case scenario for the specified attack, thus establishing a proof for model robustness.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Correctness of Codes using Deep Learning Platforms</title>
      <link>https://foreverzyh.github.io/project/project_3/</link>
      <pubDate>Fri, 01 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/project/project_3/</guid>
      <description>&lt;h4&gt;This thrust studies the increasingly important problem of ensuring the correctness of deep learning training and inference codes built on top of TensorFlow/PyTorch.&lt;/h4&gt;
&lt;p&gt;Deep learning models have made remarkable advancements in solving complex and diverse problems.
These models manifest as training and inference codes developed on platforms like TensorFlow and PyTorch.
These codes require a programming paradigm that differs significantly from traditional ones.
As a result, new defects have emerged in deep learning codes, and we need to rethink the problem of verifying the correctness of these codes and providing provable fixes to these defects.&lt;/p&gt;
&lt;p&gt;In my research on the provable guarantees for the correctness of deep learning codes, I ask: &lt;em&gt;What are these new defects of deep learning codes? Detecting and fixing which brings the largest benefit to the deep learning community?&lt;/em&gt;
The first question leads me to conduct a pioneering empirical study of novel defects in deep learning codes.
The second question guides my work on ensuring deep learning codes are free of &lt;em&gt;numerical defects&lt;/em&gt;, which often manifest as &lt;code&gt;NaN&lt;/code&gt; or &lt;code&gt;INF&lt;/code&gt; during the computation of neural networks.
These defects, which widely exist in deep learning codes, significantly impair the model&amp;rsquo;s effectiveness and may lead to system crashes.&lt;/p&gt;
&lt;div style=&#34;display: flex; justify-content: center; align-items: center;&#34;&gt;
&lt;figure&gt;
  &lt;img src=&#34;https://foreverzyh.github.io/uploads/code_debar.png&#34; alt=&#34;DEBAR&#34; style=&#34;width:100%; padding: 10px;&#34;&gt;
  &lt;figcaption&gt;DEBAR: A static analyzer over the computational graph.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
  &lt;img src=&#34;https://foreverzyh.github.io/uploads/code_ranum.png&#34; alt=&#34;RANUM&#34; style=&#34;width:100%; padding: 10px;&#34;&gt;
  &lt;figcaption&gt;RANUM: A framework for reliability assurance.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Papers:&lt;/strong&gt; &lt;a href=&#34;https://foreverzyh.github.io/uploads/An%20Empirical%20Study%20on%20TensorFlow%20Program%20Bugs-issta18.pdf&#34;&gt;A empirical study (ISSTA2018)&lt;/a&gt;, &lt;a href=&#34;https://foreverzyh.github.io/uploads/Detecting%20Numerical%20Bugs%20in%20Neural%20Network%20Architectures-esec-fse20.pdf&#34;&gt;DEBAR (FSE2020)&lt;/a&gt;, &lt;a href=&#34;http://linyil.com/res/pub/ICSE_2023_li_reliability.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RANUM (ICSE2023)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key ideas:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A pioneering empirical study of defects in deep learning codes.&lt;/li&gt;
&lt;li&gt;Abstract interpretation techniques for verifying deep learning codes without numerical defects.&lt;/li&gt;
&lt;li&gt;A framework for reliability assurance for deep learning codes against numerical defects by providing failure-exhibiting tests and provable fix suggestions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;A pioneering empirical study&lt;/strong&gt;
Motivated by the need to study emerging defects in deep learning codes, I conducted a pioneering empirical study on the root causes, symptoms, and fixing strategies of these new defects.
This empirical study not only serves as a bedrock of my work in this thrust but also inspires a multitude of endeavors in the research community,
e.g., approaches for detecting new defects identified by our study, refinement of our taxonomy and extension to deep learning system in general.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Verification of the absence of numerical defects&lt;/strong&gt;
My work, DEBAR, aims to detect numerical defects in deep learning codes.
It achieves this by either constructing a proof demonstrating the absence of numerical defects within the codes or identifying suspicious computational graph nodes that may potentially have numerical defects.
To construct such proof, we design refined yet scalable abstract domains, &lt;em&gt;tensor partitioning&lt;/em&gt; and &lt;em&gt;interval with affine equality relation&lt;/em&gt;, to over-approximate the output range of each node in the computational graph, representing the deep learning code.
If a node receives an invalid range or its output overflows, we will report this node as suspicious. And if no such node exists in the code, we then get a correctness proof owing to the soundness of abstract interpretation.
DEBAR has been applied to deep learning code maintained by TensorFlow and successfully detected numerical defects in them (&lt;a href=&#34;https://github.com/tensorflow/models/pull/8221&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;defect 1&lt;/a&gt;, &lt;a href=&#34;https://github.com/tensorflow/models/pull/8223&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;defect 2&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Provable fixes to numerical defects&lt;/strong&gt;
Once the defects are exposed, developers need to fix these defects. However, we found that developers either provide no fixes or heuristic fixes that do not eliminate these defects, such as reducing the learning rate.
Therefore, our work, RANUM, aims to synthesize provable fixes to these defects for developers automatically.
We apply interval preconditions to clipping the range of defective nodes in the computational graph.
Striking the right balance with these interval preconditions is critical; if the preconditions are overly strong, they can hinder the model&amp;rsquo;s performance, whereas if they are too weak, they may still result in numerical defects.
We propose an abstraction optimization algorithm to find the weakest precondition that provably eliminates the defects.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>I give a joint talk with my advisor Aws at [WFVML 23](https://www.ml-verification.com/invited-speakers).</title>
      <link>https://foreverzyh.github.io/news/2023_07_28/</link>
      <pubDate>Fri, 28 Jul 2023 14:00:00 -1000</pubDate>
      <guid>https://foreverzyh.github.io/news/2023_07_28/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ICML 2023, 2022, 2021, Reviewer</title>
      <link>https://foreverzyh.github.io/services/icml/</link>
      <pubDate>Sun, 23 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/services/icml/</guid>
      <description></description>
    </item>
    
    <item>
      <title>WFVML 2023, Reviewer</title>
      <link>https://foreverzyh.github.io/services/wfvml/</link>
      <pubDate>Tue, 18 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/services/wfvml/</guid>
      <description></description>
    </item>
    
    <item>
      <title>FoMLAS 2023, 2022, 2021, Program Committee</title>
      <link>https://foreverzyh.github.io/services/fomlas/</link>
      <pubDate>Mon, 17 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/services/fomlas/</guid>
      <description></description>
    </item>
    
    <item>
      <title>I start my summer internship at AWS CodeWhisperer.</title>
      <link>https://foreverzyh.github.io/news/2023_05_22/</link>
      <pubDate>Mon, 22 May 2023 08:30:00 -0500</pubDate>
      <guid>https://foreverzyh.github.io/news/2023_05_22/</guid>
      <description></description>
    </item>
    
    <item>
      <title>I pass my Ph.D. Preliminary Exam.</title>
      <link>https://foreverzyh.github.io/news/2023_05_09/</link>
      <pubDate>Tue, 09 May 2023 12:00:00 -0600</pubDate>
      <guid>https://foreverzyh.github.io/news/2023_05_09/</guid>
      <description></description>
    </item>
    
    <item>
      <title>BANDS 2023, Program Committee</title>
      <link>https://foreverzyh.github.io/services/bands/</link>
      <pubDate>Fri, 05 May 2023 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/services/bands/</guid>
      <description></description>
    </item>
    
    <item>
      <title>VMCAI 2023, Artifact Evaluation Committee</title>
      <link>https://foreverzyh.github.io/services/vmcai/</link>
      <pubDate>Mon, 16 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/services/vmcai/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Reliability Assurance for Deep Neural Network Architectures Against Numerical Defects</title>
      <link>https://foreverzyh.github.io/publication/se_ml_icse2023/</link>
      <pubDate>Thu, 01 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/publication/se_ml_icse2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>top reviewer</title>
      <link>https://foreverzyh.github.io/award/top_reviewer22/</link>
      <pubDate>Wed, 30 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/award/top_reviewer22/</guid>
      <description></description>
    </item>
    
    <item>
      <title>BagFlip: A Certified Defense Against Data Poisoning</title>
      <link>https://foreverzyh.github.io/publication/pl_ml_neurips2022/</link>
      <pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/publication/pl_ml_neurips2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Overwatch: Learning Patterns in Code Edit Sequences</title>
      <link>https://foreverzyh.github.io/publication/pl_oopsla2023/</link>
      <pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/publication/pl_oopsla2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>two sigma</title>
      <link>https://foreverzyh.github.io/award/twosigma/</link>
      <pubDate>Wed, 16 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/award/twosigma/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Certified Robustness to Programmable Transformations in LSTMs</title>
      <link>https://foreverzyh.github.io/publication/pl_ml_emnlp2021/</link>
      <pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/publication/pl_ml_emnlp2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CAV 2021, Artifact Evaluation Committee</title>
      <link>https://foreverzyh.github.io/services/cav/</link>
      <pubDate>Thu, 01 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/services/cav/</guid>
      <description></description>
    </item>
    
    <item>
      <title>distinguished paper</title>
      <link>https://foreverzyh.github.io/award/distinguished_paper/</link>
      <pubDate>Mon, 30 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/award/distinguished_paper/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Detecting Numerical Bugs in Neural Network Architectures</title>
      <link>https://foreverzyh.github.io/publication/se_ml_fse2020/</link>
      <pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/publication/se_ml_fse2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Robustness to Programmable String Transformations via Augmented Abstract Training</title>
      <link>https://foreverzyh.github.io/publication/pl_ml_icml2020/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/publication/pl_ml_icml2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>undergrad thesis</title>
      <link>https://foreverzyh.github.io/award/undergrad_thesis/</link>
      <pubDate>Sun, 30 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/award/undergrad_thesis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>scholarship senseTime</title>
      <link>https://foreverzyh.github.io/award/scholarship_2019/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/award/scholarship_2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Empirical Study on TensorFlow Program Bugs</title>
      <link>https://foreverzyh.github.io/publication/se_ml_issta2018/</link>
      <pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/publication/se_ml_issta2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>scholarship Suzhou Industrial Park</title>
      <link>https://foreverzyh.github.io/award/scholarship_2018/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/award/scholarship_2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>acm icpc Ho-Chi-Minh City 2017</title>
      <link>https://foreverzyh.github.io/award/acm_icpc_hcm_2017/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/award/acm_icpc_hcm_2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>acm icpc xian 2017</title>
      <link>https://foreverzyh.github.io/award/acm_icpc_xian_2017/</link>
      <pubDate>Sun, 01 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/award/acm_icpc_xian_2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>scholarship Schlumberger</title>
      <link>https://foreverzyh.github.io/award/scholarship_2017/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/award/scholarship_2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>acm icpc chinafinal 2016</title>
      <link>https://foreverzyh.github.io/award/acm_icpc_chinafinal_2016/</link>
      <pubDate>Fri, 02 Dec 2016 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/award/acm_icpc_chinafinal_2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>acm icpc Yangon 2016</title>
      <link>https://foreverzyh.github.io/award/acm_icpc_yangon_2016/</link>
      <pubDate>Thu, 01 Dec 2016 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/award/acm_icpc_yangon_2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>acm icpc dalian 2016</title>
      <link>https://foreverzyh.github.io/award/acm_icpc_dalian_2016/</link>
      <pubDate>Tue, 01 Nov 2016 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/award/acm_icpc_dalian_2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>scholarship iPinYou</title>
      <link>https://foreverzyh.github.io/award/scholarship_2016/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/award/scholarship_2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>acm icpc hefei 2015</title>
      <link>https://foreverzyh.github.io/award/acm_icpc_hefei_2015/</link>
      <pubDate>Sun, 01 Nov 2015 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/award/acm_icpc_hefei_2015/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Wrote a [tutorial](/uploads/CPT.html) on requesting CPT for international students in the CS department at UW-Madison.</title>
      <link>https://foreverzyh.github.io/services/cpt/</link>
      <pubDate>Wed, 01 Mar 2000 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/services/cpt/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://foreverzyh.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://foreverzyh.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
